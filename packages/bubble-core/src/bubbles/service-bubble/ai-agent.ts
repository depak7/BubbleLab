import { z, type ZodTypeAny } from 'zod';
import { ServiceBubble } from '../../types/service-bubble-class.js';
import type { BubbleContext } from '../../types/bubble.js';
import {
  CredentialType,
  BUBBLE_CREDENTIAL_OPTIONS,
  RECOMMENDED_MODELS,
} from '@bubblelab/shared-schemas';
import { StateGraph, MessagesAnnotation } from '@langchain/langgraph';
import { ChatOpenAI } from '@langchain/openai';
import { ChatAnthropic } from '@langchain/anthropic';
import {
  HumanMessage,
  AIMessage,
  ToolMessage,
  AIMessageChunk,
} from '@langchain/core/messages';
import type { BaseMessage } from '@langchain/core/messages';
import { DynamicStructuredTool } from '@langchain/core/tools';
import { AvailableModels } from '@bubblelab/shared-schemas';
import {
  AvailableTools,
  type AvailableTool,
} from '../../types/available-tools.js';
import { BubbleFactory } from '../../bubble-factory.js';
import type { BubbleName, BubbleResult } from '@bubblelab/shared-schemas';
import type { StreamingEvent } from '@bubblelab/shared-schemas';
import { ConversationMessageSchema } from '@bubblelab/shared-schemas';
import {
  extractAndStreamThinkingTokens,
  formatFinalResponse,
  generationsToMessageContent,
  isGarbageResponse,
} from '../../utils/agent-formatter.js';
import { isAIMessage, isAIMessageChunk } from '@langchain/core/messages';
import { HarmBlockThreshold, HarmCategory } from '@google/generative-ai';
import { SafeGeminiChat } from '../../utils/safe-gemini-chat.js';
import {
  zodSchemaToJsonString,
  buildJsonSchemaInstruction,
} from '../../utils/zod-schema.js';
import {
  getCapability,
  type CapabilityRuntimeContext,
} from '../../capabilities/index.js';
import {
  applyCapabilityPostprocessing,
  applyCapabilityPreprocessing,
} from './capability-pipeline.js';
// Define tool hook context - provides access to messages and tool call details
export type ToolHookContext = {
  toolName: AvailableTool;
  toolInput: unknown;
  toolOutput?: BubbleResult<unknown>; // Only available in afterToolCall
  messages: BaseMessage[];
  bubbleContext?: BubbleContext; // Access to executionMeta, variableId, etc.
};

// Tool hooks can modify the entire messages array (including system prompt)
export type ToolHookAfter = (
  context: ToolHookContext
) => Promise<{ messages: BaseMessage[]; shouldStop?: boolean }>;

export type ToolHookBefore = (context: ToolHookContext) => Promise<{
  messages: BaseMessage[];
  toolInput: Record<string, any>;
  shouldSkip?: boolean;
  skipMessage?: string;
}>;

// Context for afterLLMCall hook - provides access to messages and the last AI response
export type AfterLLMCallContext = {
  messages: BaseMessage[];
  lastAIMessage: AIMessage | AIMessageChunk;
  hasToolCalls: boolean;
};

// Hook that runs after LLM responds but before routing decision
// Can modify messages and force continuation back to LLM instead of ending
export type AfterLLMCallHook = (context: AfterLLMCallContext) => Promise<{
  messages: BaseMessage[];
  continueToAgent?: boolean; // If true, routes back to agent instead of ending
}>;

// Type for streaming callback function
export type StreamingCallback = (event: StreamingEvent) => Promise<void> | void;

// Define backup model configuration schema
const BackupModelConfigSchema = z.object({
  model: AvailableModels.describe(
    'Backup AI model to use if the primary model fails (format: provider/model-name).'
  ),
  temperature: z
    .number()
    .min(0)
    .max(2)
    .optional()
    .describe(
      'Temperature for backup model. If not specified, uses primary model temperature.'
    ),
  maxTokens: z
    .number()
    .positive()
    .optional()
    .describe(
      'Max tokens for backup model. If not specified, uses primary model maxTokens.'
    ),
  reasoningEffort: z
    .enum(['low', 'medium', 'high'])
    .optional()
    .describe(
      'Reasoning effort for backup model. If not specified, uses primary model reasoningEffort.'
    ),
  maxRetries: z
    .number()
    .int()
    .min(0)
    .max(10)
    .optional()
    .describe(
      'Max retries for backup model. If not specified, uses primary model maxRetries.'
    ),
});

// Define model configuration
const ModelConfigSchema = z.object({
  model: AvailableModels.describe(
    'AI model to use (format: provider/model-name).'
  ),
  temperature: z
    .number()
    .min(0)
    .max(2)
    .default(1)
    .describe(
      'Temperature for response randomness (0 = deterministic, 2 = very random)'
    ),
  maxTokens: z
    .number()
    .positive()
    .optional()
    .default(64000)
    .describe(
      'Maximum number of tokens to generate in response, keep at default of 40000 unless the response is expected to be certain length'
    ),
  reasoningEffort: z
    .enum(['low', 'medium', 'high'])
    .optional()
    .describe(
      'Reasoning effort for model. If not specified, uses primary model reasoningEffort.'
    ),
  maxRetries: z
    .number()
    .int()
    .min(0)
    .max(10)
    .default(3)
    .describe(
      'Maximum number of retries for API calls (default: 3). Useful for handling transient errors like 503 Service Unavailable.'
    ),
  provider: z
    .array(z.string())
    .optional()
    .describe('Providers for ai agent (open router only).'),
  jsonMode: z
    .boolean()
    .default(false)
    .describe(
      'When true, returns clean JSON response, you must provide the exact JSON schema in the system prompt'
    ),
  backupModel: BackupModelConfigSchema.default({
    model: RECOMMENDED_MODELS.FAST,
  })
    .optional()
    .describe('Backup model configuration to use if the primary model fails.'),
});

// Define tool configuration for pre-registered tools
const ToolConfigSchema = z.object({
  name: AvailableTools.describe(
    'Name of the tool type or tool bubble to enable for the AI agent'
  ),
  credentials: z
    .record(z.nativeEnum(CredentialType), z.string())
    .default({})
    .optional()
    .describe(
      'Credential types to use for the tool bubble (injected at runtime)'
    ),
  config: z
    .record(z.string(), z.unknown())
    .optional()
    .describe('Configuration for the tool or tool bubble'),
});

// Define custom tool schema for runtime-defined tools
const CustomToolSchema = z.object({
  name: z
    .string()
    .min(1)
    .describe('Unique name for your custom tool (e.g., "calculate-tax")'),
  description: z
    .string()
    .min(1)
    .describe(
      'Description of what the tool does - helps the AI know when to use it'
    ),
  schema: z
    .union([
      z.record(z.string(), z.unknown()),
      z.custom<z.ZodTypeAny>(
        (val) => val && typeof val === 'object' && '_def' in val
      ),
    ])
    .describe(
      'Zod schema object defining the tool parameters. Can be either a plain object (e.g., { amount: z.number() }) or a Zod object directly (e.g., z.object({ amount: z.number() })).'
    ),
  func: z
    .function()
    .args(z.record(z.string(), z.unknown()))
    .returns(z.promise(z.unknown()))
    .describe(
      'Async function that executes the tool logic. Receives params matching the schema and returns a result.'
    ),
});

// Define image input schemas - supports both base64 data and URLs
const Base64ImageSchema = z.object({
  type: z.literal('base64').default('base64'),
  data: z
    .string()
    .describe('Base64 encoded image data (without data:image/... prefix)'),
  mimeType: z
    .string()
    .default('image/png')
    .describe('MIME type of the image (e.g., image/png, image/jpeg)'),
  description: z
    .string()
    .optional()
    .describe('Optional description or context for the image'),
});

const UrlImageSchema = z.object({
  type: z.literal('url'),
  url: z.string().url().describe('URL to the image (http/https)'),
  description: z
    .string()
    .optional()
    .describe('Optional description or context for the image'),
});

const ImageInputSchema = z.discriminatedUnion('type', [
  Base64ImageSchema,
  UrlImageSchema,
]);

// Schema for the expected JSON output structure - accepts either a Zod schema or a JSON schema string
const ExpectedOutputSchema = z.union([
  z.custom<ZodTypeAny>((val) => val?._def !== undefined),
  z.string(),
]);

/** Type for conversation history messages - enables KV cache optimization */
export type ConversationMessage = z.infer<typeof ConversationMessageSchema>;

// Schema for a single capability configuration on the AI agent
const CapabilityConfigSchema = z.object({
  id: z
    .string()
    .min(1)
    .describe('Capability ID (e.g., "google-doc-knowledge-base")'),
  inputs: z
    .record(
      z.string(),
      z.union([z.string(), z.number(), z.boolean(), z.array(z.string())])
    )
    .default({})
    .describe('Input parameter values for this capability'),
  credentials: z
    .record(z.nativeEnum(CredentialType), z.string())
    .default({})
    .optional()
    .describe('Capability-specific credentials (injected at runtime)'),
});

// Define the parameters schema for the AI Agent bubble
const AIAgentParamsSchema = z.object({
  message: z
    .string()
    .min(1, 'Message is required')
    .describe('The message or question to send to the AI agent'),
  images: z
    .array(ImageInputSchema)
    .default([])
    .describe(
      'Array of base64 encoded images to include with the message (for multimodal AI models). Example: [{type: "base64", data: "base64...", mimeType: "image/png", description: "A beautiful image of a cat"}] or [{type: "url", url: "https://example.com/image.png", description: "A beautiful image of a cat"}]'
    ),
  conversationHistory: z
    .array(ConversationMessageSchema)
    .optional()
    .describe(
      'Previous conversation messages for multi-turn conversations. When provided, messages are sent as separate turns to enable KV cache optimization. Format: [{role: "user", content: "..."}, {role: "assistant", content: "..."}, ...]'
    ),
  systemPrompt: z
    .string()
    .default('You are a helpful AI assistant')
    .describe(
      'System prompt that defines the AI agents behavior and personality'
    ),
  name: z
    .string()
    .default('AI Agent')
    .optional()
    .describe('A friendly name for the AI agent'),
  model: ModelConfigSchema.default({
    model: RECOMMENDED_MODELS.FAST,
    temperature: 1,
    maxTokens: 65536,
    maxRetries: 3,
    jsonMode: false,
  }).describe(
    'AI model configuration including provider, temperature, and tokens, retries, and json mode. Always include this.'
  ),
  tools: z
    .array(ToolConfigSchema)
    .default([])
    .describe(
      'Array of tool config objects: [{ name: "web-search-tool" }, { name: "web-scrape-tool" }]. Each object requires a "name" field. Available tool names: web-search-tool, web-scrape-tool, web-crawl-tool, web-extract-tool, instagram-tool. If using image models, set tools to []'
    ),
  customTools: z
    .array(CustomToolSchema)
    .default([])
    .optional()
    .describe(
      'Array of custom runtime-defined tools with their own schemas and functions. Use this to add domain-specific tools without pre-registration. Example: [{ name: "calculate-tax", description: "Calculates sales tax", schema: { amount: z.number() }, func: async (input) => {...} }]'
    ),
  maxIterations: z
    .number()
    .positive()
    .min(4)
    .default(40)
    .describe(
      'Maximum number of iterations for the agent workflow, 5 iterations per turn of conversation'
    ),
  credentials: z
    .record(z.nativeEnum(CredentialType), z.string())
    .optional()
    .describe(
      'Object mapping credential types to values (injected at runtime)'
    ),
  streaming: z
    .boolean()
    .default(false)
    .describe(
      'Enable real-time streaming of tokens, tool calls, and iteration progress'
    ),
  capabilities: z
    .array(CapabilityConfigSchema)
    .default([])
    .optional()
    .describe(
      'Capabilities that extend the agent with bundled tools, prompts, and credentials. Example: [{ id: "google-doc-knowledge-base", inputs: { docId: "your-doc-id" } }]'
    ),
  expectedOutputSchema: ExpectedOutputSchema.optional().describe(
    'Zod schema or JSON schema string that defines the expected structure of the AI response. When provided, automatically enables JSON mode and instructs the AI to output in the exact format. Example: z.object({ summary: z.string(), items: z.array(z.object({ name: z.string(), score: z.number() })) })'
  ),
  memoryEnabled: z
    .boolean()
    .default(true)
    .describe(
      'Enable persistent memory across conversations. When true, the agent can recall and save information about people, topics, and events between conversations.'
    ),
  enableSlackHistory: z
    .boolean()
    .default(false)
    .describe(
      'Enable Slack thread history injection. When true, the agent receives full conversation history from Slack threads including user names, timezones, and images.'
    ),
  // Note: beforeToolCall and afterToolCall are function hooks added via TypeScript interface
  // They cannot be part of the Zod schema but are available in the params
});
const AIAgentResultSchema = z.object({
  response: z
    .string()
    .describe(
      'The AI agents final response to the user message. For text responses, returns plain text. If JSON mode is enabled, returns a JSON string. For image generation models (like gemini-2.5-flash-image-preview), returns base64-encoded image data with data URI format (data:image/png;base64,...)'
    ),
  reasoning: z
    .string()
    .nullable()
    .optional()
    .describe(
      'The reasoning/thinking tokens from the model (if available). Present for deep research models and reasoning models.'
    ),
  toolCalls: z
    .array(
      z.object({
        tool: z.string().describe('Name of the tool that was called'),
        input: z.unknown().describe('Input parameters passed to the tool'),
        output: z.unknown().describe('Output returned by the tool'),
      })
    )
    .describe('Array of tool calls made during the conversation'),
  iterations: z
    .number()
    .describe('Number of back-and-forth iterations in the agent workflow'),
  totalCost: z
    .number()
    .optional()
    .describe(
      'Total cost in USD for this request (includes tokens + web search for deep research models)'
    ),
  error: z
    .string()
    .describe('Error message of the run, undefined if successful'),
  success: z
    .boolean()
    .describe('Whether the agent execution completed successfully'),
});

export type AIAgentParams = z.input<typeof AIAgentParamsSchema> & {
  // Optional hooks for intercepting tool calls
  beforeToolCall?: ToolHookBefore;
  afterToolCall?: ToolHookAfter;
  // Hook that runs after LLM responds but before routing (can force retry)
  afterLLMCall?: AfterLLMCallHook;
  streamingCallback?: StreamingCallback;
};
export type AIAgentParamsParsed = z.output<typeof AIAgentParamsSchema> & {
  beforeToolCall?: ToolHookBefore;
  afterToolCall?: ToolHookAfter;
  afterLLMCall?: AfterLLMCallHook;
  streamingCallback?: StreamingCallback;
};

export type AIAgentResult = z.output<typeof AIAgentResultSchema>;

export class AIAgentBubble extends ServiceBubble<
  AIAgentParamsParsed,
  AIAgentResult
> {
  static readonly type = 'service' as const;
  static readonly service = 'ai-agent';
  static readonly authType = 'apikey' as const;
  static readonly bubbleName: BubbleName = 'ai-agent';
  static readonly schema = AIAgentParamsSchema;
  static readonly resultSchema = AIAgentResultSchema;
  static readonly shortDescription =
    'AI agent with LangGraph for tool-enabled conversations, multimodal support, and JSON mode';
  static readonly longDescription = `
    An AI agent powered by LangGraph that can use any tool bubble to answer questions.
    Use cases:
    - Add tools to enhance the AI agent's capabilities (web-search-tool, web-scrape-tool)
    - Multi-step reasoning with tool assistance
    - Tool-augmented conversations with any registered tool
    - JSON mode for structured output (strips markdown formatting)
  `;
  static readonly alias = 'agent';

  private factory: BubbleFactory;
  private beforeToolCallHook: ToolHookBefore | undefined;
  private afterToolCallHook: ToolHookAfter | undefined;
  /** Capability-scoped hooks: only fire for the capability's own tool names */
  private capabilityBeforeHooks = new Map<string, ToolHookBefore>();
  private capabilityAfterHooks = new Map<string, ToolHookAfter>();
  private afterLLMCallHook: AfterLLMCallHook | undefined;
  private streamingCallback: StreamingCallback | undefined;
  private shouldStopAfterTools = false;
  private shouldContinueToAgent = false;
  private rescueAttempts = 0;
  private static readonly MAX_RESCUE_ATTEMPTS = 1;

  constructor(
    params: AIAgentParams = {
      message: 'Hello, how are you?',
      systemPrompt: 'You are a helpful AI assistant',
      model: { model: RECOMMENDED_MODELS.FAST },
    },
    context?: BubbleContext,
    instanceId?: string
  ) {
    super(params, context, instanceId);
    this.beforeToolCallHook = params.beforeToolCall;
    this.afterToolCallHook = params.afterToolCall;
    this.afterLLMCallHook = params.afterLLMCall;
    this.streamingCallback = params.streamingCallback;
    this.factory = new BubbleFactory();
  }

  public async testCredential(): Promise<boolean> {
    // Make a test API call to the model provider
    const llm = this.initializeModel(this.params.model);

    const response = await llm.invoke(['Hello, how are you?']);
    if (response.content) {
      return true;
    }
    return false;
  }

  /**
   * Build effective model config from primary and optional backup settings
   */
  private buildModelConfig(
    primaryConfig: AIAgentParamsParsed['model'],
    backupConfig?: z.infer<typeof BackupModelConfigSchema>
  ): AIAgentParamsParsed['model'] {
    if (!backupConfig) {
      return primaryConfig;
    }

    return {
      model: backupConfig.model,
      temperature: backupConfig.temperature ?? primaryConfig.temperature,
      maxTokens: backupConfig.maxTokens ?? primaryConfig.maxTokens,
      maxRetries: backupConfig.maxRetries ?? primaryConfig.maxRetries,
      provider: primaryConfig.provider,
      jsonMode: primaryConfig.jsonMode,
      backupModel: undefined, // Don't chain backup models
    };
  }

  /**
   * Core execution logic for running the agent with a given model config
   */
  private async executeWithModel(
    modelConfig: AIAgentParamsParsed['model']
  ): Promise<AIAgentResult> {
    const {
      message,
      images,
      systemPrompt,
      tools,
      customTools,
      maxIterations,
      conversationHistory,
    } = this.params;

    // Initialize the language model
    const llm = this.initializeModel(modelConfig);

    // Initialize tools (both pre-registered and custom)
    const agentTools = await this.initializeTools(tools, customTools);

    // Create the agent graph
    const graph = await this.createAgentGraph(llm, agentTools, systemPrompt);

    // Execute the agent
    return this.executeAgent(
      graph,
      message,
      images,
      maxIterations,
      modelConfig,
      conversationHistory,
      agentTools
    );
  }

  /**
   * Modify params before execution - centralizes all param transformations
   */
  private async beforeAction(): Promise<void> {
    // Deduplicate capabilities by id â€” keep the first occurrence of each
    if (this.params.capabilities && this.params.capabilities.length > 1) {
      const seen = new Set<string>();
      this.params.capabilities = this.params.capabilities.filter((c) => {
        if (seen.has(c.id)) return false;
        seen.add(c.id);
        return true;
      });
    }

    // Enforce minimum maxTokens of 10000
    if (
      this.params.model.maxTokens === undefined ||
      this.params.model.maxTokens < 10000
    ) {
      this.params.model.maxTokens = 10000;
    }

    // Auto-enable JSON mode when expectedOutputSchema is provided
    if (this.params.expectedOutputSchema) {
      this.params.model.jsonMode = true;

      // Enhance system prompt with JSON schema instructions
      const schemaString = zodSchemaToJsonString(
        this.params.expectedOutputSchema
      );
      this.params.systemPrompt = `${this.params.systemPrompt}\n\n${buildJsonSchemaInstruction(schemaString)}`;
    }

    // Inject current UTC time into system prompt
    const now = new Date().toLocaleString('en-US', {
      weekday: 'long',
      year: 'numeric',
      month: 'long',
      day: 'numeric',
      hour: '2-digit',
      minute: '2-digit',
      timeZone: 'UTC',
      timeZoneName: 'short',
    });
    this.params.systemPrompt = `${this.params.systemPrompt}\n\n**System time (UTC):** ${now}\nIMPORTANT: The system time above is in UTC. Always interpret and present times from the user's perspective and timezone. If the user's timezone is known (from conversation history, user profile, or context), convert all times accordingly. If unknown, ask the user for their timezone before making time-sensitive decisions.`;

    // Apply capability model overrides and system prompt injections
    await applyCapabilityPreprocessing(
      this.params,
      this.context,
      this.resolveCapabilityCredentials.bind(this)
    );

    // Extract execution metadata (used for conversation history + agent memory)
    const execMeta = this.context?.executionMeta;

    // Inject Slack channel context into system prompt
    const slackChannel = execMeta?._slackChannel;
    if (slackChannel) {
      this.params.systemPrompt = `${this.params.systemPrompt}\n**Current Slack channel:** ${slackChannel}`;
    }

    // Auto-inject trigger conversation history if no explicit conversationHistory was provided
    // This enables Slack thread context to automatically flow into AI agents
    // Check both legacy path (context.triggerConversationHistory) and new path (executionMeta)
    if (!this.params.conversationHistory?.length) {
      const convHistory =
        (execMeta?.triggerConversationHistory as
          | Array<{ role: 'user' | 'assistant'; content: string }>
          | undefined) ??
        (this.context?.triggerConversationHistory as
          | Array<{ role: 'user' | 'assistant'; content: string }>
          | undefined);
      if (convHistory?.length) {
        this.params.conversationHistory = convHistory;
      }
    }

    // Memory injection â€” tools, prompt, and reflection are built externally (Pro)
    // and passed via executionMeta. This keeps all memory logic out of OSS.
    const isCapabilityAgent = this.params.name?.startsWith('Capability Agent:');

    if (!isCapabilityAgent && this.params.memoryEnabled) {
      const memoryTools = execMeta?.memoryTools;
      const memorySystemPrompt = execMeta?.memorySystemPrompt;

      if (memoryTools?.length) {
        if (!this.params.customTools) {
          this.params.customTools = [];
        }
        this.params.customTools.push(...memoryTools);
      }

      if (memorySystemPrompt) {
        this.params.systemPrompt = `${this.params.systemPrompt}\n\n---\n\n${memorySystemPrompt}`;
      }

      // Initialize callLLM for memory tools that need it (update_memory merge, reflection)
      const memoryCallLLMInit = execMeta?.memoryCallLLMInit as
        | ((callLLM: (prompt: string) => Promise<string>) => void)
        | undefined;
      if (memoryCallLLMInit) {
        const callLLM = async (prompt: string): Promise<string> => {
          const memoryAgent = new AIAgentBubble(
            {
              message: prompt,
              systemPrompt:
                'Respond concisely. Follow the instructions in the user message.',
              name: 'Capability Agent: Memory',
              model: {
                model: RECOMMENDED_MODELS.FAST,
                temperature: 0,
                maxTokens: 4096,
                maxRetries: 2,
              },
              credentials: this.params.credentials,
              maxIterations: 4,
            },
            this.context,
            'memory-agent'
          );
          const result = await memoryAgent.action();
          return result.data?.response ?? '';
        };
        memoryCallLLMInit(callLLM);
      }
    }
  }

  protected async performAction(
    context?: BubbleContext
  ): Promise<AIAgentResult> {
    // Context is available but not currently used in this implementation
    void context;

    // Apply param transformations before execution
    await this.beforeAction();

    try {
      let result: AIAgentResult;

      // Check if this is a deep research model - bypass LangChain and call OpenRouter directly
      if (this.isDeepResearchModel(this.params.model.model)) {
        console.log(
          '[AIAgent] Deep research model detected, using direct OpenRouter API'
        );
        result = await this.executeDeepResearchViaOpenRouter();
      } else {
        result = await this.executeWithModel(this.params.model);
      }

      // Append capability response additions (e.g. tips, blurbs)
      if (result.success) {
        result = await applyCapabilityPostprocessing(
          result,
          this.params,
          this.context,
          this.resolveCapabilityCredentials.bind(this)
        );
      }

      // Post-execution memory reflection â€” callback built externally (Pro)
      this.runMemoryReflectionIfNeeded(result);

      return result;
    } catch (error) {
      const errorMessage =
        error instanceof Error ? error.message : 'Unknown error';
      console.warn('[AIAgent] Execution error:', errorMessage);

      // Notify executionMeta callback for agent-level errors (e.g. PostHog tracking)
      this.context?.executionMeta?._onAgentError?.({
        error: errorMessage,
        model: this.params.model.model,
        iterations: 0,
        toolCalls: [],
        conversationHistory: this.params.conversationHistory?.map((m) => ({
          role: m.role,
          content: m.content,
        })),
        variableId: this.context?.variableId,
      });

      // Return error information but mark as recoverable
      return {
        response: `Error: ${errorMessage}`,
        success: false,
        toolCalls: [],
        error: errorMessage,
        iterations: 0,
      };
    }
  }

  /**
   * Run post-execution memory reflection if a callback was provided via executionMeta.
   * Fire-and-forget: doesn't block the response.
   */
  private runMemoryReflectionIfNeeded(result: AIAgentResult): void {
    const isCapabilityAgent = this.params.name?.startsWith('Capability Agent:');
    if (isCapabilityAgent || !result.success || !this.params.memoryEnabled) {
      return;
    }

    const execMeta = this.context?.executionMeta;
    const memoryReflectionCallback = execMeta?.memoryReflectionCallback;

    if (!memoryReflectionCallback) return;

    // Build conversation messages from the conversation history + current exchange
    const messages: Array<{ role: string; content: string }> = [];

    // Add conversation history
    if (this.params.conversationHistory?.length) {
      for (const msg of this.params.conversationHistory) {
        messages.push({ role: msg.role, content: msg.content });
      }
    }

    // Add the current user message
    messages.push({ role: 'user', content: this.params.message });

    // Add tool calls as context
    if (result.toolCalls?.length) {
      for (const tc of result.toolCalls) {
        messages.push({
          role: 'assistant',
          content: `[Used tool: ${tc.tool}] Input: ${JSON.stringify(tc.input).slice(0, 200)}`,
        });
      }
    }

    // Add the final response
    messages.push({ role: 'assistant', content: result.response });

    // Fire-and-forget
    memoryReflectionCallback(messages).catch((err) => {
      console.error('[AIAgent] Memory reflection failed:', err);
    });
  }

  protected getCredentialType(): CredentialType {
    return this.getCredentialTypeForModel(this.params.model.model);
  }

  /**
   * Get credential type for a specific model string
   */
  private getCredentialTypeForModel(model: string): CredentialType {
    const [provider] = model.split('/');
    switch (provider) {
      case 'openai':
        return CredentialType.OPENAI_CRED;
      case 'google':
        return CredentialType.GOOGLE_GEMINI_CRED;
      case 'anthropic':
        return CredentialType.ANTHROPIC_CRED;
      case 'openrouter':
        return CredentialType.OPENROUTER_CRED;
      default:
        throw new Error(`Unsupported model provider: ${provider}`);
    }
  }

  protected chooseCredential(): string | undefined {
    const { model } = this.params;
    const credentials = this.params.credentials as
      | Record<CredentialType, string>
      | undefined;
    const [provider] = model.model.split('/');

    // If no credentials were injected, throw error immediately (like PostgreSQL)
    if (!credentials || typeof credentials !== 'object') {
      throw new Error(`No ${provider.toUpperCase()} credentials provided`);
    }

    // Choose credential based on the model provider
    switch (provider) {
      case 'openai':
        return credentials[CredentialType.OPENAI_CRED];
      case 'google':
        return credentials[CredentialType.GOOGLE_GEMINI_CRED];
      case 'anthropic':
        return credentials[CredentialType.ANTHROPIC_CRED];
      case 'openrouter':
        return credentials[CredentialType.OPENROUTER_CRED];
      default:
        throw new Error(`Unsupported model provider: ${provider}`);
    }
  }

  /**
   * Check if the model is a deep research model that requires direct API call
   */
  private isDeepResearchModel(model: string): boolean {
    return (
      model === 'openrouter/openai/o3-deep-research' ||
      model === 'openrouter/openai/o4-mini-deep-research'
    );
  }

  /**
   * Execute deep research models via OpenRouter API directly
   * Bypasses LangChain since these models have compatibility issues
   */
  private async executeDeepResearchViaOpenRouter(): Promise<AIAgentResult> {
    const { message, systemPrompt, model, conversationHistory } = this.params;
    // Extract the model name without 'openrouter/' prefix
    const modelName = model.model.replace('openrouter/', '');

    const credentials = this.params.credentials as
      | Record<CredentialType, string>
      | undefined;
    const apiKey = credentials?.[CredentialType.OPENROUTER_CRED];

    if (!apiKey) {
      throw new Error(
        'OpenRouter API key is required for deep research models'
      );
    }

    // Build messages array
    const messages: Array<{ role: string; content: string }> = [];

    // Add system prompt
    if (systemPrompt) {
      messages.push({ role: 'system', content: systemPrompt });
    }

    // Add conversation history if provided
    if (conversationHistory && conversationHistory.length > 0) {
      for (const historyMsg of conversationHistory) {
        if (historyMsg.role === 'user' || historyMsg.role === 'assistant') {
          messages.push({ role: historyMsg.role, content: historyMsg.content });
        }
      }
    }

    // Add current message
    messages.push({ role: 'user', content: message });

    // Emit start event
    await this.streamingCallback?.({
      type: 'llm_start',
      data: {
        model: model.model,
        temperature: model.temperature,
      },
    });

    try {
      const response = await fetch(
        'https://openrouter.ai/api/v1/chat/completions',
        {
          method: 'POST',
          headers: {
            Authorization: `Bearer ${apiKey}`,
            'Content-Type': 'application/json',
            'HTTP-Referer': 'https://bubblelab.ai',
            'X-Title': 'BubbleLab',
          },
          body: JSON.stringify({
            model: modelName,
            messages,
            temperature: model.temperature,
            max_tokens: model.maxTokens,
            usage: { include: true },
          }),
        }
      );

      if (!response.ok) {
        const errorData = (await response.json()) as {
          error?: { message?: string };
        };
        throw new Error(
          `OpenRouter API error: ${response.status} - ${errorData?.error?.message || response.statusText}`
        );
      }

      const data = (await response.json()) as {
        id: string;
        choices: Array<{
          message: {
            role: string;
            content: string;
            reasoning?: string; // Some models return reasoning in message
          };
          finish_reason: string;
        }>;
        // OpenRouter returns usage object with cost when usage.include is true
        usage?: {
          prompt_tokens?: number;
          completion_tokens?: number;
          total_tokens?: number;
          cost?: number;
        };
      };

      const finalResponse = data.choices?.[0]?.message?.content || '';
      const reasoning = data.choices?.[0]?.message?.reasoning;
      const totalCost = data.usage?.cost;

      // Log total cost if available
      if (typeof totalCost === 'number' && this.context?.logger) {
        this.context.logger.logTokenUsage(
          {
            usage: totalCost,
            service: CredentialType.OPENROUTER_CRED,
            unit: 'total_cost_usd',
            subService: model.model as CredentialType,
          },
          `Deep Research total cost: $${totalCost.toFixed(4)}`,
          {
            bubbleName: 'ai-agent',
            variableId: this.context?.variableId,
            operationType: 'bubble_execution',
          }
        );
      }

      // Emit complete event
      await this.streamingCallback?.({
        type: 'llm_complete',
        data: {
          messageId: data.id,
          content: finalResponse,
          totalTokens: data.usage?.total_tokens,
        },
      });

      return {
        response: finalResponse,
        reasoning,
        toolCalls: [],
        iterations: 1,
        totalCost,
        error: '',
        success: true,
      };
    } catch (error) {
      const errorMessage =
        error instanceof Error ? error.message : 'Unknown error';

      await this.streamingCallback?.({
        type: 'error',
        data: {
          error: errorMessage,
          recoverable: false,
        },
      });

      return {
        response: `Deep Research Error: ${errorMessage}`,
        toolCalls: [],
        iterations: 0,
        error: errorMessage,
        success: false,
      };
    }
  }

  private initializeModel(modelConfig: AIAgentParamsParsed['model']) {
    const { model, temperature, maxTokens, maxRetries } = modelConfig;
    const slashIndex = model.indexOf('/');
    const provider = model.substring(0, slashIndex);
    const modelName = model.substring(slashIndex + 1);
    const reasoningEffort = modelConfig.reasoningEffort;

    // Get credential based on the modelConfig's provider (not this.params.model)
    const credentials = this.params.credentials as
      | Record<CredentialType, string>
      | undefined;

    if (!credentials || typeof credentials !== 'object') {
      throw new Error(`No ${provider.toUpperCase()} credentials provided`);
    }

    let apiKey: string | undefined;
    switch (provider) {
      case 'openai':
        apiKey = credentials[CredentialType.OPENAI_CRED];
        break;
      case 'google':
        apiKey = credentials[CredentialType.GOOGLE_GEMINI_CRED];
        break;
      case 'anthropic':
        apiKey = credentials[CredentialType.ANTHROPIC_CRED];
        break;
      case 'openrouter':
        apiKey = credentials[CredentialType.OPENROUTER_CRED];
        break;
      default:
        throw new Error(`Unsupported model provider: ${provider}`);
    }

    if (!apiKey) {
      throw new Error(`No credential found for provider: ${provider}`);
    }

    // Enable streaming if streamingCallback is provided
    const enableStreaming = !!this.streamingCallback;

    // Default to 3 retries if not specified
    const retries = maxRetries ?? 3;

    switch (provider) {
      case 'openai':
        return new ChatOpenAI({
          model: modelName,
          temperature,
          maxTokens,
          apiKey,
          ...(reasoningEffort && {
            reasoning: {
              effort: reasoningEffort,
              summary: 'auto',
            },
          }),
          streaming: enableStreaming,
          maxRetries: retries,
        });
      case 'google': {
        const thinkingConfig = reasoningEffort
          ? {
              includeThoughts: reasoningEffort ? true : false,
              thinkingBudget:
                reasoningEffort === 'low'
                  ? 1025
                  : reasoningEffort === 'medium'
                    ? 5000
                    : 10000,
            }
          : undefined;
        return new SafeGeminiChat({
          model: modelName,
          temperature,
          maxOutputTokens: maxTokens,
          ...(thinkingConfig && { thinkingConfig }),
          apiKey,
          // 3.0 pro preview does breaks with streaming, disabled temporarily until fixed
          streaming: false,
          maxRetries: retries,
          // Disable all safety filters to prevent candidateContent.parts.reduce errors
          // when Gemini blocks content and returns candidates without content field
          safetySettings: [
            {
              category: HarmCategory.HARM_CATEGORY_HARASSMENT,
              threshold: HarmBlockThreshold.BLOCK_NONE,
            },
            {
              category: HarmCategory.HARM_CATEGORY_HATE_SPEECH,
              threshold: HarmBlockThreshold.BLOCK_NONE,
            },
            {
              category: HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
              threshold: HarmBlockThreshold.BLOCK_NONE,
            },
            {
              category: HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
              threshold: HarmBlockThreshold.BLOCK_NONE,
            },
          ],
        });
      }
      case 'anthropic': {
        // Configure Anthropic "thinking" only when reasoning is enabled.
        // Anthropic's API does not allow `budget_tokens` when thinking is disabled.
        const thinkingConfig =
          reasoningEffort != null
            ? {
                type: 'enabled' as const,
                budget_tokens:
                  reasoningEffort === 'low'
                    ? 1025
                    : reasoningEffort === 'medium'
                      ? 5000
                      : 10000,
              }
            : undefined;

        const isThinking = thinkingConfig != null;
        const anthropicModel = new ChatAnthropic({
          model: modelName,
          // Anthropic requires temperature=1 when thinking is enabled
          temperature: isThinking ? 1 : temperature,
          anthropicApiKey: apiKey,
          maxTokens,
          streaming: true,
          apiKey,
          ...(thinkingConfig && { thinking: thinkingConfig }),
          maxRetries: retries,
        });
        // LangChain 0.3.x defaults topP to -1 and only clears it for
        // hardcoded model names (opus-4-1, sonnet-4-5, haiku-4-5).
        // Newer models like opus-4-6 aren't whitelisted, so force-clear it.
        // When thinking is enabled, topP must stay at -1 (sentinel for "not set")
        // because Anthropic rejects topP with thinking.
        if (!isThinking) {
          anthropicModel.topP = undefined;
        }
        return anthropicModel;
      }
      case 'openrouter':
        console.log('openrouter', modelName);
        return new ChatOpenAI({
          model: modelName,
          __includeRawResponse: true,
          temperature,
          maxTokens,
          apiKey,
          streaming: enableStreaming,
          maxRetries: retries,
          configuration: {
            baseURL: 'https://openrouter.ai/api/v1',
          },
          modelKwargs: {
            provider: {
              order: this.params.model.provider,
            },
            reasoning: {
              effort: reasoningEffort ?? 'medium',
              exclude: false,
            },
          },
        });
      default:
        throw new Error(`Unsupported model provider: ${provider}`);
    }
  }

  private async initializeTools(
    toolConfigs: AIAgentParamsParsed['tools'],
    customToolConfigs: AIAgentParamsParsed['customTools'] = []
  ): Promise<DynamicStructuredTool[]> {
    const tools: DynamicStructuredTool[] = [];
    await this.factory.registerDefaults();

    // First, initialize custom tools
    for (const customTool of customToolConfigs) {
      try {
        console.log(
          `ðŸ› ï¸ [AIAgent] Initializing custom tool: ${customTool.name}`
        );

        // Handle both plain object and Zod object schemas
        let schema: z.ZodTypeAny;
        if (
          customTool.schema &&
          typeof customTool.schema === 'object' &&
          '_def' in customTool.schema
        ) {
          // Already a Zod schema object, use it directly
          schema = customTool.schema as z.ZodTypeAny;
        } else {
          // Plain object, convert to Zod object
          schema = z.object(customTool.schema as z.ZodRawShape) as z.ZodTypeAny;
        }

        const dynamicTool = new DynamicStructuredTool({
          name: customTool.name,
          description: customTool.description,
          schema: schema,
          func: customTool.func as (input: any) => Promise<any>,
        } as any);

        tools.push(dynamicTool);
      } catch (error) {
        console.error(
          `Error initializing custom tool '${customTool.name}':`,
          error
        );
        // Continue with other tools even if one fails
        continue;
      }
    }

    // Then, initialize pre-registered tools from factory
    for (const toolConfig of toolConfigs) {
      try {
        const ToolBubbleClass = this.factory.get(toolConfig.name as BubbleName);

        if (!ToolBubbleClass) {
          if (this.context && this.context.logger) {
            this.context.logger.warn(
              `Tool bubble '${toolConfig.name}' not found in factory. This tool will not be used.`
            );
          }
          console.warn(
            `Tool bubble '${toolConfig.name}' not found in factory. This tool will not be used.`
          );
          continue;
        }

        // Check if it's a tool bubble (has toAgentTool method)
        if (!('type' in ToolBubbleClass) || ToolBubbleClass.type !== 'tool') {
          console.warn(`Bubble '${toolConfig.name}' is not a tool bubble`);
          continue;
        }

        // Convert to LangGraph tool and add to tools array
        if (!ToolBubbleClass.toolAgent) {
          console.warn(
            `Tool bubble '${toolConfig.name}' does not have a toolAgent method`
          );
          continue;
        }

        // Get tool's credential requirements and pass relevant credentials from AI agent
        const toolCredentialOptions =
          BUBBLE_CREDENTIAL_OPTIONS[toolConfig.name as BubbleName] || [];
        const toolCredentials: Record<string, string> = {};

        // Pass AI agent's credentials to tools that need them
        for (const credType of toolCredentialOptions) {
          if (this.params.credentials && this.params.credentials[credType]) {
            toolCredentials[credType] = this.params.credentials[credType];
          }
        }

        // Merge with any explicitly provided tool credentials (explicit ones take precedence)
        const finalToolCredentials = {
          ...toolCredentials,
          ...(toolConfig.credentials || {}),
        };

        console.log(
          `ðŸ” [AIAgent] Passing credentials to ${toolConfig.name}:`,
          Object.keys(finalToolCredentials)
        );

        const langGraphTool = ToolBubbleClass.toolAgent(
          finalToolCredentials,
          toolConfig.config || {},
          this.context
        );

        const dynamicTool = new DynamicStructuredTool({
          name: langGraphTool.name,
          description: langGraphTool.description,
          schema: langGraphTool.schema as unknown as z.ZodTypeAny,
          func: langGraphTool.func as (input: any) => Promise<any>,
        } as any);

        tools.push(dynamicTool);
      } catch (error) {
        console.error(`Error initializing tool '${toolConfig.name}':`, error);
        // Continue with other tools even if one fails
        continue;
      }
    }

    // 3. Capability tools
    const caps = this.params.capabilities ?? [];
    if (caps.length > 1) {
      // Multi-capability: register master-level tools directly on the master agent
      for (const capConfig of caps) {
        const capDef = getCapability(capConfig.id);
        if (!capDef) continue;

        const masterTools = capDef.metadata.tools.filter((t) => t.masterTool);
        if (masterTools.length === 0) continue;

        try {
          const ctx: CapabilityRuntimeContext = {
            credentials: this.resolveCapabilityCredentials(capDef, capConfig),
            inputs: capConfig.inputs ?? {},
            bubbleContext: this.context,
          };
          const toolFuncs = capDef.createTools(ctx);
          const logger = this.context?.logger;

          for (const toolMeta of masterTools) {
            const func = toolFuncs[toolMeta.name];
            if (!func) continue;

            const { variableId: capToolVariableId, uniqueId: capToolUniqueId } =
              this.resolveCapabilityToolNode(toolMeta.name);

            const capToolContext: BubbleContext | undefined = this.context
              ? {
                  ...this.context,
                  variableId: capToolVariableId,
                  currentUniqueId: capToolUniqueId,
                  __uniqueIdCounters__: {},
                }
              : undefined;

            const wrappedFunc = async (
              input: Record<string, unknown>
            ): Promise<unknown> => {
              logger?.logBubbleExecution(
                capToolVariableId,
                toolMeta.name,
                toolMeta.name,
                input
              );
              try {
                ctx.bubbleContext = capToolContext;
                const result = await func(input);
                ctx.bubbleContext = this.context;
                logger?.logBubbleExecutionComplete(
                  capToolVariableId,
                  toolMeta.name,
                  toolMeta.name,
                  result
                );
                return result;
              } catch (error) {
                ctx.bubbleContext = this.context;
                logger?.logBubbleExecutionComplete(
                  capToolVariableId,
                  toolMeta.name,
                  toolMeta.name,
                  { success: false, error: String(error) }
                );
                throw error;
              }
            };

            const toolSchema = this.jsonSchemaToZod(toolMeta.parameterSchema);
            const dynamicTool = new DynamicStructuredTool({
              name: toolMeta.name,
              description: toolMeta.description,
              schema: toolSchema,
              func: wrappedFunc as (
                input: Record<string, unknown>
              ) => Promise<unknown>,
            } as any);

            tools.push(dynamicTool);
            console.log(
              `ðŸ”§ [AIAgent] Registered master-level tool: ${toolMeta.name} (from ${capConfig.id})`
            );
          }

          // Register hooks for master-level tools
          if (capDef.hooks?.beforeToolCall) {
            for (const t of masterTools) {
              this.capabilityBeforeHooks.set(
                t.name,
                capDef.hooks.beforeToolCall
              );
            }
          }
          if (capDef.hooks?.afterToolCall) {
            for (const t of masterTools) {
              this.capabilityAfterHooks.set(t.name, capDef.hooks.afterToolCall);
            }
          }
        } catch (error) {
          console.error(
            `Error initializing master-level tools for capability '${capConfig.id}':`,
            error
          );
          continue;
        }
      }

      // Register use-capability delegation tool
      const capIds = caps.map((c) => c.id);
      tools.push(
        new DynamicStructuredTool({
          name: 'use-capability',
          description:
            'Delegate a task to a specialized capability. The capability has its own tools and context to handle the task.',
          schema: z.object({
            capabilityId: z
              .enum(capIds as [string, ...string[]])
              .describe('Which capability to delegate to'),
            task: z
              .string()
              .describe(
                'Clear description of what to do. Include any relevant context from the conversation. Always include information about the users timezone and current time.'
              ),
          }),
          func: async (input: Record<string, unknown>) => {
            const capabilityId = input.capabilityId as string;
            const task = input.task as string;
            const capConfig = caps.find((c) => c.id === capabilityId);
            const capDef = getCapability(capabilityId);
            if (!capConfig || !capDef)
              return { error: `Capability "${capabilityId}" not found` };

            const subAgent = new AIAgentBubble(
              {
                message: task,
                systemPrompt: '', // capability's systemPrompt fills this via beforeAction
                name: `Capability Agent: ${capDef.metadata.name}`,
                model: { ...this.params.model },
                capabilities: [capConfig], // single cap = eager load in sub-agent
                credentials: this.params.credentials,
              },
              this.context,
              `capability-${capabilityId}`
            );

            const result = await subAgent.action();
            if (!result.success) {
              return { success: false, error: result.error };
            }
            return { success: true, response: result.data?.response };
          },
        } as any)
      );
      console.log(
        `ðŸ”§ [AIAgent] Multi-capability delegation mode: registered use-capability tool for [${capIds.join(', ')}]`
      );
    } else {
      // Single capability or none: register tools directly as before
      for (const capConfig of caps) {
        const capDef = getCapability(capConfig.id);
        if (!capDef) {
          console.warn(
            `[AIAgent] Capability '${capConfig.id}' not found in registry. Skipping.`
          );
          continue;
        }

        try {
          // Shared ctx captured by all tool funcs via closure â€” we mutate bubbleContext per-tool
          const ctx: CapabilityRuntimeContext = {
            credentials: this.resolveCapabilityCredentials(capDef, capConfig),
            inputs: capConfig.inputs ?? {},
            bubbleContext: this.context,
          };
          const toolFuncs = capDef.createTools(ctx);
          const logger = this.context?.logger;

          for (const toolMeta of capDef.metadata.tools) {
            const func = toolFuncs[toolMeta.name];
            if (!func) continue;

            // Resolve this capability tool's variableId and uniqueId from the dependency graph
            const { variableId: capToolVariableId, uniqueId: capToolUniqueId } =
              this.resolveCapabilityToolNode(toolMeta.name);

            // Build a per-tool child context so inner bubbles resolve under this capability tool
            const capToolContext: BubbleContext | undefined = this.context
              ? {
                  ...this.context,
                  variableId: capToolVariableId,
                  currentUniqueId: capToolUniqueId,
                  __uniqueIdCounters__: {},
                }
              : undefined;

            // Wrap the tool function with execution logging and per-tool context
            const wrappedFunc = async (
              input: Record<string, unknown>
            ): Promise<unknown> => {
              logger?.logBubbleExecution(
                capToolVariableId,
                toolMeta.name,
                toolMeta.name,
                input
              );

              try {
                // Swap ctx.bubbleContext so inner bubbles resolve under the capability tool node
                ctx.bubbleContext = capToolContext;
                const result = await func(input);
                ctx.bubbleContext = this.context; // restore

                logger?.logBubbleExecutionComplete(
                  capToolVariableId,
                  toolMeta.name,
                  toolMeta.name,
                  result
                );
                return result;
              } catch (error) {
                ctx.bubbleContext = this.context; // restore on error
                logger?.logBubbleExecutionComplete(
                  capToolVariableId,
                  toolMeta.name,
                  toolMeta.name,
                  { success: false, error: String(error) }
                );
                throw error;
              }
            };

            // Convert JSON schema back to Zod for DynamicStructuredTool
            const toolSchema = this.jsonSchemaToZod(toolMeta.parameterSchema);

            const dynamicTool = new DynamicStructuredTool({
              name: toolMeta.name,
              description: toolMeta.description,
              schema: toolSchema,
              func: wrappedFunc as (
                input: Record<string, unknown>
              ) => Promise<unknown>,
            } as any);

            tools.push(dynamicTool);
            console.log(
              `ðŸ”§ [AIAgent] Registered capability tool: ${toolMeta.name} (from ${capConfig.id}, variableId: ${capToolVariableId})`
            );
          }

          // Wire capability-level hooks scoped to this capability's tool names
          const capToolNames = capDef.metadata.tools.map((t) => t.name);
          if (capDef.hooks?.beforeToolCall) {
            for (const name of capToolNames) {
              this.capabilityBeforeHooks.set(name, capDef.hooks.beforeToolCall);
            }
          }
          if (capDef.hooks?.afterToolCall) {
            for (const name of capToolNames) {
              this.capabilityAfterHooks.set(name, capDef.hooks.afterToolCall);
            }
          }
        } catch (error) {
          console.error(
            `Error initializing capability '${capConfig.id}':`,
            error
          );
          continue;
        }
      }
    }

    return tools;
  }

  /**
   * Resolves credentials for a capability by pulling from the bubble-level
   * credentials (this.params.credentials) for each type the capability requires,
   * then overlaying any explicitly set capConfig.credentials on top.
   */
  private resolveCapabilityCredentials(
    capDef: {
      metadata: {
        requiredCredentials: CredentialType[];
        optionalCredentials?: CredentialType[];
      };
    },
    capConfig: { credentials?: Record<string, string> }
  ): Partial<Record<CredentialType, string>> {
    const resolved: Partial<Record<CredentialType, string>> = {};

    // Pull from bubble-level credentials for each type the capability needs
    const allCredTypes = [
      ...capDef.metadata.requiredCredentials,
      ...(capDef.metadata.optionalCredentials ?? []),
    ];
    for (const credType of allCredTypes) {
      if (this.params.credentials && this.params.credentials[credType]) {
        resolved[credType] = this.params.credentials[credType];
      }
    }

    // Overlay any explicitly set capability-level credentials (take precedence)
    if (capConfig.credentials) {
      for (const [key, value] of Object.entries(capConfig.credentials)) {
        resolved[key as CredentialType] = value;
      }
    }

    return resolved;
  }

  /**
   * Resolves the variableId and uniqueId for a capability tool by walking the dependency graph.
   * Finds the child node matching the given tool name under the current ai-agent node.
   * Falls back to -999/empty string if the dependency graph is unavailable or the tool is not found.
   */
  private resolveCapabilityToolNode(toolName: string): {
    variableId: number;
    uniqueId: string;
  } {
    const graph = this.context?.dependencyGraph;
    const currentId = this.context?.currentUniqueId;
    if (!graph) return { variableId: -999, uniqueId: '' };

    // Find the current ai-agent node in the dependency graph
    const findByUniqueId = (
      node: {
        uniqueId?: string;
        dependencies?: Array<{
          uniqueId?: string;
          name: string;
          variableId?: number;
          dependencies?: unknown[];
        }>;
      },
      target: string
    ): {
      dependencies?: Array<{
        uniqueId?: string;
        name: string;
        variableId?: number;
      }>;
    } | null => {
      if (node.uniqueId === target) return node;
      for (const child of node.dependencies || []) {
        const found = findByUniqueId(child as typeof node, target);
        if (found) return found;
      }
      return null;
    };

    const parentNode = currentId ? findByUniqueId(graph, currentId) : graph;
    if (!parentNode?.dependencies) return { variableId: -999, uniqueId: '' };

    // Find the child node matching the capability tool name
    const matchingChild = parentNode.dependencies.find(
      (c) => c.name === toolName
    );
    return {
      variableId: matchingChild?.variableId ?? -999,
      uniqueId: matchingChild?.uniqueId ?? '',
    };
  }

  /**
   * Converts a JSON Schema object to a Zod schema for DynamicStructuredTool.
   * Handles common JSON Schema types used by capability tool definitions.
   */
  private jsonSchemaToZod(
    jsonSchema: Record<string, unknown>
  ): z.ZodObject<z.ZodRawShape> {
    const properties = (
      jsonSchema as { properties?: Record<string, Record<string, unknown>> }
    ).properties;
    const required = (jsonSchema as { required?: string[] }).required ?? [];

    if (!properties || Object.keys(properties).length === 0) {
      return z.object({});
    }

    const shape: z.ZodRawShape = {};
    for (const [key, prop] of Object.entries(properties)) {
      let fieldSchema: z.ZodTypeAny;

      switch (prop.type) {
        case 'string':
          fieldSchema = z.string();
          if (prop.description)
            fieldSchema = fieldSchema.describe(prop.description as string);
          break;
        case 'number':
        case 'integer':
          fieldSchema = z.number();
          if (prop.description)
            fieldSchema = fieldSchema.describe(prop.description as string);
          break;
        case 'boolean':
          fieldSchema = z.boolean();
          if (prop.description)
            fieldSchema = fieldSchema.describe(prop.description as string);
          break;
        case 'array':
          fieldSchema = z.array(z.unknown());
          if (prop.description)
            fieldSchema = fieldSchema.describe(prop.description as string);
          break;
        default:
          fieldSchema = z.unknown();
          if (prop.description)
            fieldSchema = fieldSchema.describe(prop.description as string);
          break;
      }

      if (!required.includes(key)) {
        fieldSchema = fieldSchema.optional();
      }

      shape[key] = fieldSchema;
    }

    return z.object(shape);
  }

  /**
   * Custom tool execution node that supports hooks
   */
  private async executeToolsWithHooks(
    state: typeof MessagesAnnotation.State,
    tools: DynamicStructuredTool[]
  ): Promise<{ messages: BaseMessage[] }> {
    const { messages } = state;
    const lastMessage = messages[messages.length - 1] as AIMessage;
    const toolCalls = lastMessage.tool_calls || [];

    const toolMessages: BaseMessage[] = [];
    let currentMessages = [...messages];
    let hooksModifiedMessages = false;

    // Reset stop flag at the start of tool execution
    this.shouldStopAfterTools = false;

    // Execute each tool call
    for (const toolCall of toolCalls) {
      const tool = tools.find((t) => t.name === toolCall.name);
      if (!tool) {
        console.warn(`Tool ${toolCall.name} not found`);
        const availableToolNames = tools.map((t) => t.name).join(', ');
        const errorContent = `Error: Tool "${toolCall.name}" not found. Available tools: ${availableToolNames}`;
        const startTime = Date.now();

        // Send tool_start event (include variableId for console tracking)
        this.streamingCallback?.({
          type: 'tool_call_start',
          data: {
            tool: toolCall.name,
            input: toolCall.args,
            callId: toolCall.id!,
            variableId: this.context?.variableId,
          },
        });

        // Notify executionMeta callback (e.g. Slack thinking-message status updates)
        this.context?.executionMeta?._onToolCallStart?.(
          toolCall.name,
          toolCall.args
        );

        // Send tool_complete event with error
        this.streamingCallback?.({
          type: 'tool_call_complete',
          data: {
            callId: toolCall.id!,
            input: toolCall.args as { input: string },
            tool: toolCall.name,
            output: { error: errorContent },
            duration: Date.now() - startTime,
            variableId: this.context?.variableId,
          },
        });

        // Notify executionMeta callback for tool call errors (e.g. PostHog tracking)
        this.context?.executionMeta?._onToolCallError?.({
          toolName: toolCall.name,
          toolInput: toolCall.args,
          error: errorContent,
          errorType: 'not_found',
          variableId: this.context?.variableId,
          model: this.params.model.model,
        });

        const errorMessage = new ToolMessage({
          content: errorContent,
          tool_call_id: toolCall.id!,
        });
        toolMessages.push(errorMessage);
        currentMessages = [...currentMessages, errorMessage];
        continue;
      }

      const startTime = Date.now();
      try {
        // Call beforeToolCall hook â€” capability-scoped hook takes priority, then global
        const beforeHook =
          this.capabilityBeforeHooks.get(toolCall.name) ??
          this.beforeToolCallHook;
        const hookResult_before = await beforeHook?.({
          toolName: toolCall.name as AvailableTool,
          toolInput: toolCall.args,
          messages: currentMessages,
          bubbleContext: this.context,
        });

        this.streamingCallback?.({
          type: 'tool_call_start',
          data: {
            tool: toolCall.name,
            input: toolCall.args,
            callId: toolCall.id!,
            variableId: this.context?.variableId,
          },
        });

        // Notify executionMeta callback (e.g. Slack thinking-message status updates)
        this.context?.executionMeta?._onToolCallStart?.(
          toolCall.name,
          toolCall.args
        );

        // If hook returns modified messages/toolInput, apply them
        if (hookResult_before) {
          if (hookResult_before.messages) {
            currentMessages = hookResult_before.messages;
            hooksModifiedMessages = true;
          }
          toolCall.args = hookResult_before.toolInput;

          // If hook requests skipping, create synthetic ToolMessage and stop agent loop
          if (hookResult_before.shouldSkip) {
            const skipMsg = new ToolMessage({
              content:
                hookResult_before.skipMessage || 'Tool execution was skipped.',
              tool_call_id: toolCall.id!,
            });
            toolMessages.push(skipMsg);
            currentMessages = [...currentMessages, skipMsg];
            this.shouldStopAfterTools = true;
            continue;
          }
        }

        // Execute the tool
        const toolOutput = await tool.invoke(toolCall.args);

        // Create tool message
        const toolMessage = new ToolMessage({
          content:
            typeof toolOutput === 'string'
              ? toolOutput
              : JSON.stringify(toolOutput),
          tool_call_id: toolCall.id!,
        });

        toolMessages.push(toolMessage);
        currentMessages = [...currentMessages, toolMessage];

        // Call afterToolCall hook â€” capability-scoped hook takes priority, then global
        const afterHook =
          this.capabilityAfterHooks.get(toolCall.name) ??
          this.afterToolCallHook;
        const hookResult_after = await afterHook?.({
          toolName: toolCall.name as AvailableTool,
          toolInput: toolCall.args,
          toolOutput,
          messages: currentMessages,
          bubbleContext: this.context,
        });

        // If hook returns modified messages, update current messages
        if (hookResult_after) {
          if (hookResult_after.messages) {
            currentMessages = hookResult_after.messages;
            hooksModifiedMessages = true;
          }
          // Check if hook wants to stop execution
          if (hookResult_after.shouldStop === true) {
            this.shouldStopAfterTools = true;
          }
        }
        this.streamingCallback?.({
          type: 'tool_call_complete',
          data: {
            callId: toolCall.id!,
            input: toolCall.args as { input: string },
            tool: toolCall.name,
            output: toolOutput,
            duration: Date.now() - startTime,
            variableId: this.context?.variableId,
          },
        });
      } catch (error) {
        console.error(`Error executing tool ${toolCall.name}:`, error);
        const errorContent = `Error: ${error instanceof Error ? error.message : 'Unknown error'}`;
        const errorMessage = new ToolMessage({
          content: errorContent,
          tool_call_id: toolCall.id!,
        });
        toolMessages.push(errorMessage);
        currentMessages = [...currentMessages, errorMessage];

        // Send tool_complete event even on failure so frontend can track it properly
        this.streamingCallback?.({
          type: 'tool_call_complete',
          data: {
            callId: toolCall.id!,
            input: toolCall.args as { input: string },
            tool: toolCall.name,
            output: { error: errorContent },
            duration: Date.now() - startTime,
            variableId: this.context?.variableId,
          },
        });

        // Notify executionMeta callback for tool call errors (e.g. PostHog tracking)
        this.context?.executionMeta?._onToolCallError?.({
          toolName: toolCall.name,
          toolInput: toolCall.args,
          error: errorContent,
          errorType: 'execution_error',
          variableId: this.context?.variableId,
          model: this.params.model.model,
        });
      }
    }

    // If hooks modified existing messages, return the full array so LangGraph's
    // messagesStateReducer can merge by ID (update existing + append new).
    // Otherwise, return only new tool messages for efficiency.
    if (hooksModifiedMessages) {
      return { messages: currentMessages };
    }
    return { messages: toolMessages };
  }

  private async createAgentGraph(
    llm: ChatOpenAI | SafeGeminiChat | ChatAnthropic,
    tools: DynamicStructuredTool[],
    systemPrompt: string
  ) {
    // Define the agent node
    const agentNode = async ({ messages }: typeof MessagesAnnotation.State) => {
      // systemPrompt is already enhanced by beforeAction() if expectedOutputSchema was provided
      // Use cache_control for Anthropic models to cache the system prompt across iterations
      const isAnthropic = llm instanceof ChatAnthropic;
      const systemMessage = isAnthropic
        ? new HumanMessage({
            content: [
              {
                type: 'text' as const,
                text: systemPrompt,
                cache_control: { type: 'ephemeral' as const },
              },
            ],
          })
        : new HumanMessage(systemPrompt);
      const allMessages = [systemMessage, ...messages];

      // Helper function for exponential backoff with jitter
      const exponentialBackoff = (attemptNumber: number): Promise<void> => {
        // Base delay: 1 second, exponentially increases (1s, 2s, 4s, 8s, ...)
        const baseDelay = 1000;
        const maxDelay = 32000; // Cap at 32 seconds
        const delay = Math.min(
          baseDelay * Math.pow(2, attemptNumber - 1),
          maxDelay
        );

        // Add jitter (random Â±25% variation) to prevent thundering herd
        const jitter = delay * 0.25 * (Math.random() - 0.5);
        const finalDelay = delay + jitter;

        return new Promise((resolve) => setTimeout(resolve, finalDelay));
      };

      // Shared onFailedAttempt callback to avoid duplication
      const onFailedAttempt = async (error: any) => {
        const attemptNumber = error.attemptNumber;
        const retriesLeft = error.retriesLeft;

        // Check if this is a candidateContent error
        const errorMessage = error.message || String(error);
        if (
          errorMessage.includes('candidateContent') ||
          errorMessage.includes('parts.reduce') ||
          errorMessage.includes('undefined is not an object')
        ) {
          this.context?.logger?.error(
            `[AIAgent] Gemini candidateContent error detected (attempt ${attemptNumber}). This indicates blocked/empty content from Gemini API.`
          );
        }

        this.context?.logger?.warn(
          `[AIAgent] LLM call failed (attempt ${attemptNumber}/${this.params.model.maxRetries}). Retries left: ${retriesLeft}. Error: ${error.message}`
        );

        // Optionally emit streaming event for retry
        if (this.streamingCallback) {
          await this.streamingCallback({
            type: 'error',
            data: {
              error: `Retry attempt ${attemptNumber}/${this.params.model.maxRetries}: ${error.message}`,
              recoverable: retriesLeft > 0,
            },
          });
        }

        // Wait with exponential backoff before retrying
        if (retriesLeft > 0) {
          await exponentialBackoff(attemptNumber);
        }
      };

      // If we have tools, bind them to the LLM, then add retry logic
      // IMPORTANT: Must bind tools FIRST, then add retry - not the other way around
      const modelWithTools =
        tools.length > 0
          ? llm.bindTools(tools).withRetry({
              stopAfterAttempt: this.params.model.maxRetries,
              onFailedAttempt,
            })
          : llm.withRetry({
              stopAfterAttempt: this.params.model.maxRetries,
              onFailedAttempt,
            });

      try {
        // Use streaming if streamingCallback is provided
        if (this.streamingCallback) {
          const messageId = `msg-${Date.now()}-${Math.random().toString(36).substring(2, 9)}`;

          // Use invoke with callbacks for streaming
          const response = await modelWithTools.invoke(allMessages, {
            callbacks: [
              {
                handleLLMStart: async (): Promise<void> => {
                  await this.streamingCallback?.({
                    type: 'llm_start',
                    data: {
                      model: this.params.model.model,
                      temperature: this.params.model.temperature,
                    },
                  });
                },
                handleLLMEnd: async (output): Promise<void> => {
                  // Extract thinking tokens from different model providers
                  const thinking = extractAndStreamThinkingTokens(output);
                  if (thinking) {
                    await this.streamingCallback?.({
                      type: 'think',
                      data: {
                        content: thinking,
                        messageId,
                      },
                    });
                  }
                  const content = formatFinalResponse(
                    generationsToMessageContent(output.generations.flat()),
                    this.params.model.model
                  ).response;
                  await this.streamingCallback?.({
                    type: 'llm_complete',
                    data: {
                      messageId,
                      content: content,
                      totalTokens:
                        output.llmOutput?.usage_metadata?.total_tokens,
                    },
                  });
                },
              },
            ],
          });

          return { messages: [response] };
        } else {
          // Non-streaming fallback
          const response = await modelWithTools.invoke(allMessages);
          return { messages: [response] };
        }
      } catch (error) {
        // Catch candidateContent errors that slip through SafeGeminiChat
        const errorMessage =
          error instanceof Error ? error.message : String(error);

        if (
          errorMessage.includes('candidateContent') ||
          errorMessage.includes('parts.reduce') ||
          errorMessage.includes('undefined is not an object')
        ) {
          console.error(
            '[AIAgent] Caught candidateContent error in agentNode:',
            errorMessage
          );

          // Return error as AIMessage instead of crashing
          return {
            messages: [
              new AIMessage({
                content: `[Gemini Error] Unable to generate response due to content filtering. Error: ${errorMessage}`,
                additional_kwargs: {
                  finishReason: 'ERROR',
                  error: errorMessage,
                },
              }),
            ],
          };
        }

        // Rethrow other errors
        throw error;
      }
    };

    // Node that runs after agent to check afterLLMCall hook before routing
    const afterLLMCheckNode = async ({
      messages,
    }: typeof MessagesAnnotation.State) => {
      // Reset the flag at the start
      this.shouldContinueToAgent = false;

      // Get the last AI message
      const lastMessage = messages[messages.length - 1] as
        | AIMessage
        | AIMessageChunk;

      const hasToolCalls = !!(
        lastMessage.tool_calls && lastMessage.tool_calls.length > 0
      );

      // Built-in rescue: detect garbage response after tool use
      if (!hasToolCalls && isGarbageResponse(lastMessage.content)) {
        const hadToolUse = messages.some((m) => m instanceof ToolMessage);
        if (
          hadToolUse &&
          this.rescueAttempts < AIAgentBubble.MAX_RESCUE_ATTEMPTS
        ) {
          this.rescueAttempts++;
          console.warn(
            `[AIAgent] Garbage response detected ("${String(lastMessage.content).substring(0, 50)}"), attempting rescue (${this.rescueAttempts}/${AIAgentBubble.MAX_RESCUE_ATTEMPTS})`
          );
          this.shouldContinueToAgent = true;
          return {
            messages: [
              new HumanMessage(
                'Your last response was empty or invalid. Please provide a clear, helpful response summarizing what you did and the results.'
              ),
            ],
          };
        }
      }

      // Only call hook if we're about to end (no tool calls) and hook is provided
      if (!hasToolCalls && this.afterLLMCallHook) {
        console.log(
          '[AIAgent] No tool calls detected, calling afterLLMCall hook'
        );

        const hookResult = await this.afterLLMCallHook({
          messages,
          lastAIMessage: lastMessage,
          hasToolCalls,
        });

        // If hook wants to continue to agent, set flag and return modified messages
        if (hookResult.continueToAgent) {
          console.log('[AIAgent] afterLLMCall hook requested retry to agent');
          this.shouldContinueToAgent = true;
          // Return the modified messages from the hook
          // We need to return only the new messages to append
          const newMessages = hookResult.messages.slice(messages.length);
          return { messages: newMessages };
        }
      }

      // No modifications needed
      return { messages: [] };
    };

    // Define conditional edge function after LLM check
    const shouldContinueAfterLLMCheck = ({
      messages,
    }: typeof MessagesAnnotation.State) => {
      // First check if afterLLMCall hook requested continuing to agent
      if (this.shouldContinueToAgent) {
        return 'agent';
      }

      // Find the last AI message (could be followed by human messages from hook)
      const aiMessages: (AIMessage | AIMessageChunk)[] = [];
      for (const msg of messages) {
        if (isAIMessage(msg)) {
          aiMessages.push(msg);
        } else if (
          'tool_calls' in msg &&
          (msg as AIMessageChunk).constructor?.name === 'AIMessageChunk'
        ) {
          aiMessages.push(msg as AIMessageChunk);
        }
      }
      const lastAIMessage = aiMessages[aiMessages.length - 1];

      // Check if the last AI message has tool calls
      if (lastAIMessage?.tool_calls && lastAIMessage.tool_calls.length > 0) {
        return 'tools';
      }
      return '__end__';
    };

    // Define conditional edge after tools to check if we should stop
    const shouldContinueAfterTools = () => {
      // Check if the afterToolCall hook requested stopping
      if (this.shouldStopAfterTools) {
        return '__end__';
      }
      // Check for pending approval signal from sub-agent (shared via executionMeta).
      // In multi-capability mode the master and sub-agent share the same BubbleContext,
      // so a sub-agent setting _pendingApproval is visible here.
      const execMeta = this.context?.executionMeta;
      if (execMeta?._pendingApproval) {
        this.shouldStopAfterTools = true;
        return '__end__';
      }
      // Otherwise continue back to agent
      return 'agent';
    };

    // Build the graph
    const graph = new StateGraph(MessagesAnnotation).addNode(
      'agent',
      agentNode
    );

    if (tools.length > 0) {
      // Use custom tool node with hooks support
      const toolNode = async (state: typeof MessagesAnnotation.State) => {
        return await this.executeToolsWithHooks(state, tools);
      };

      graph
        .addNode('tools', toolNode)
        .addNode('afterLLMCheck', afterLLMCheckNode)
        .addEdge('__start__', 'agent')
        .addEdge('agent', 'afterLLMCheck')
        .addConditionalEdges('afterLLMCheck', shouldContinueAfterLLMCheck)
        .addConditionalEdges('tools', shouldContinueAfterTools);
    } else {
      // Even without tools, add the afterLLMCheck node for hook support
      graph
        .addNode('afterLLMCheck', afterLLMCheckNode)
        .addEdge('__start__', 'agent')
        .addEdge('agent', 'afterLLMCheck')
        .addConditionalEdges('afterLLMCheck', shouldContinueAfterLLMCheck);
    }

    return graph.compile();
  }

  private async executeAgent(
    graph: ReturnType<typeof StateGraph.prototype.compile>,
    message: string,
    images: AIAgentParamsParsed['images'],
    maxIterations: number,
    modelConfig: AIAgentParamsParsed['model'],
    conversationHistory?: AIAgentParamsParsed['conversationHistory'],
    tools?: DynamicStructuredTool[]
  ): Promise<AIAgentResult> {
    const jsonMode = modelConfig.jsonMode;
    const toolCalls: AIAgentResult['toolCalls'] = [];
    let iterations = 0;

    try {
      // Build messages array starting with conversation history (for KV cache optimization)
      const initialMessages: BaseMessage[] = [];

      // Resume from saved agent state (lossless â€” preserves tool_calls, etc.)
      const resumeExecMeta = this.context?.executionMeta;
      const resumeState = resumeExecMeta?._resumeAgentState;

      if (resumeState && Array.isArray(resumeState) && resumeState.length > 0) {
        const { mapStoredMessagesToChatMessages } = await import(
          '@langchain/core/messages'
        );
        const restored = mapStoredMessagesToChatMessages(
          resumeState as unknown as Parameters<
            typeof mapStoredMessagesToChatMessages
          >[0]
        );
        // Collect existing tool_call_ids that already have ToolMessage responses
        const existingToolResultIds = new Set<string>();
        for (const msg of restored) {
          if (msg._getType() === 'tool') {
            const tm = msg as ToolMessage;
            if (tm.tool_call_id) existingToolResultIds.add(tm.tool_call_id);
          }
        }

        // Find AIMessages with unresolved tool_calls (no matching ToolMessage)
        // The LAST such AIMessage is the one that triggered approval
        let approvalAiMsgIndex = -1;
        for (let i = restored.length - 1; i >= 0; i--) {
          const msg = restored[i];
          if (msg._getType() === 'ai') {
            const aiMsg = msg as AIMessage;
            const pending = aiMsg.tool_calls?.filter(
              (tc) => tc.id && !existingToolResultIds.has(tc.id)
            );
            if (pending?.length) {
              approvalAiMsgIndex = i;
              break;
            }
          }
        }

        // Build a lookup of available tools by name for direct execution
        const toolsByName = new Map<string, DynamicStructuredTool>();
        if (tools) {
          for (const t of tools) toolsByName.set(t.name, t);
        }

        // Process all AIMessages: execute or add synthetic results for unresolved tool_calls
        // We iterate in order and insert ToolMessages right after their AIMessage
        const repairedMessages: BaseMessage[] = [];
        for (let i = 0; i < restored.length; i++) {
          repairedMessages.push(restored[i]);
          const msg = restored[i];
          if (msg._getType() !== 'ai') continue;

          const aiMsg = msg as AIMessage;
          const pendingCalls = aiMsg.tool_calls?.filter(
            (tc) => tc.id && !existingToolResultIds.has(tc.id)
          );
          if (!pendingCalls?.length) continue;

          for (const tc of pendingCalls) {
            if (i === approvalAiMsgIndex && toolsByName.has(tc.name)) {
              // This is the approval-triggering AIMessage â€” execute the tool directly
              // This bypasses beforeToolCall (no re-approval check) and gives real results
              try {
                console.log(
                  `[AIAgent] Resume: executing approved tool "${tc.name}" directly`
                );
                const tool = toolsByName.get(tc.name)!;
                const result = await tool.invoke(tc.args);
                const content =
                  typeof result === 'string' ? result : JSON.stringify(result);
                repairedMessages.push(
                  new ToolMessage({ content, tool_call_id: tc.id! })
                );
              } catch (err) {
                console.warn(
                  `[AIAgent] Resume: direct tool execution failed for "${tc.name}":`,
                  err
                );
                repairedMessages.push(
                  new ToolMessage({
                    content: `Tool execution failed: ${err instanceof Error ? err.message : String(err)}`,
                    tool_call_id: tc.id!,
                  })
                );
              }
            } else {
              // Safety net: synthetic result for any other unresolved tool_calls
              repairedMessages.push(
                new ToolMessage({
                  content:
                    'This action has been approved by the user. You may now execute this tool.',
                  tool_call_id: tc.id!,
                })
              );
            }
            existingToolResultIds.add(tc.id!);
          }
        }

        initialMessages.push(...repairedMessages);
      } else if (conversationHistory && conversationHistory.length > 0) {
        // Normal path: lossy ConversationMessage[] â†’ BaseMessage[]
        // This enables KV cache optimization by keeping previous turns as separate messages
        for (const historyMsg of conversationHistory) {
          switch (historyMsg.role) {
            case 'user':
              initialMessages.push(new HumanMessage(historyMsg.content));
              break;
            case 'assistant':
              initialMessages.push(new AIMessage(historyMsg.content));
              break;
            case 'tool':
              // Tool messages require a tool_call_id
              if (historyMsg.toolCallId) {
                initialMessages.push(
                  new ToolMessage({
                    content: historyMsg.content,
                    tool_call_id: historyMsg.toolCallId,
                    name: historyMsg.name,
                  })
                );
              }
              break;
          }
        }
      }

      // Create the current human message with text and optional images
      let humanMessage: HumanMessage;

      if (images && images.length > 0) {
        console.log(
          '[AIAgent] Creating multimodal message with',
          images.length,
          'images'
        );

        // Create multimodal content array
        const content: Array<{
          type: string;
          text?: string;
          image_url?: { url: string };
        }> = [{ type: 'text', text: message }];

        // Add images to content
        for (const image of images) {
          let imageUrl: string;

          if (image.type === 'base64') {
            // Base64 encoded image
            imageUrl = `data:${image.mimeType};base64,${image.data}`;
          } else {
            // URL image - fetch and convert to base64 for Google Gemini compatibility
            try {
              console.log('[AIAgent] Fetching image from URL:', image.url);
              const response = await fetch(image.url);
              if (!response.ok) {
                throw new Error(
                  `Failed to fetch image: ${response.status} ${response.statusText}`
                );
              }

              const arrayBuffer = await response.arrayBuffer();
              const base64Data = Buffer.from(arrayBuffer).toString('base64');

              // Detect MIME type from response or default to PNG
              const contentType =
                response.headers.get('content-type') || 'image/png';
              imageUrl = `data:${contentType};base64,${base64Data}`;

              console.log(
                '[AIAgent] Successfully converted URL image to base64'
              );
            } catch (error) {
              console.error('[AIAgent] Error fetching image from URL:', error);
              throw new Error(
                `Failed to load image from URL ${image.url}: ${error instanceof Error ? error.message : 'Unknown error'}`
              );
            }
          }

          content.push({
            type: 'image_url',
            image_url: { url: imageUrl },
          });

          // Add image description if provided
          if (image.description) {
            content.push({
              type: 'text',
              text: `Image description: ${image.description}`,
            });
          }
        }

        humanMessage = new HumanMessage({ content });
      } else {
        // Text-only message
        humanMessage = new HumanMessage(message);
      }

      // Add the current message to the conversation
      initialMessages.push(humanMessage);

      const result = await graph.invoke(
        { messages: initialMessages },
        { recursionLimit: maxIterations }
      );

      console.log('[AIAgent] Graph execution completed');
      console.log('[AIAgent] Total messages:', result.messages.length);
      iterations = result.messages.length;

      // Extract tool calls from messages and track individual LLM calls
      // Store tool calls temporarily to match with their responses
      const toolCallMap = new Map<string, { name: string; args: unknown }>();

      for (let i = 0; i < result.messages.length; i++) {
        const msg = result.messages[i];
        if (
          msg instanceof AIMessage ||
          (msg instanceof AIMessageChunk && msg.tool_calls)
        ) {
          const typedToolCalls = msg.tool_calls;
          // Log and track tool calls
          for (const toolCall of typedToolCalls || []) {
            toolCallMap.set(toolCall.id!, {
              name: toolCall.name,
              args: toolCall.args,
            });

            console.log(
              '[AIAgent] Tool call:',
              toolCall.name,
              'with args:',
              toolCall.args
            );
          }
        } else if (msg instanceof ToolMessage) {
          // Match tool response to its call
          const toolCall = toolCallMap.get(msg.tool_call_id);
          if (toolCall) {
            // Parse content if it's a JSON string
            let output = msg.content;
            if (typeof output === 'string') {
              try {
                output = JSON.parse(output);
              } catch {
                // Keep as string if not valid JSON
              }
            }

            console.log(
              '[AIAgent] Tool output preview:',
              typeof output === 'string'
                ? output.substring(0, 100) + '...'
                : JSON.stringify(output).substring(0, 100) + '...'
            );

            toolCalls.push({
              tool: toolCall.name,
              input: toolCall.args,
              output,
            });
          }
        }
      }

      // Get the final AI message response
      console.log('[AIAgent] Filtering AI messages...');
      const aiMessages = result.messages.filter(
        (msg: any) => isAIMessage(msg) || isAIMessageChunk(msg)
      );
      console.log('[AIAgent] Found', aiMessages.length, 'AI messages');
      const finalMessage = aiMessages[aiMessages.length - 1] as
        | AIMessage
        | AIMessageChunk;

      if (finalMessage?.additional_kwargs?.finishReason === 'SAFETY_BLOCKED') {
        throw new Error(
          `[Gemini Error] Unable to generate a response. Please increase maxTokens in model configuration or try again with a different model.`
        );
      }
      // Check for MAX_TOKENS finish reason
      if (finalMessage?.additional_kwargs?.finishReason === 'MAX_TOKENS') {
        throw new Error(
          'Response was truncated due to max tokens limit. Please increase maxTokens in model configuration.'
        );
      }

      // Track token usage from ALL AI messages (not just the final one)
      // This is critical for multi-iteration workflows where the agent calls tools multiple times
      let totalInputTokens = 0;
      let totalOutputTokens = 0;
      let totalTokensSum = 0;

      for (const msg of result.messages) {
        if (
          msg instanceof AIMessage ||
          (msg instanceof AIMessageChunk && msg.usage_metadata)
        ) {
          totalInputTokens +=
            (msg as AIMessage | AIMessageChunk).usage_metadata?.input_tokens ||
            0;
          totalOutputTokens +=
            (msg as AIMessage | AIMessageChunk).usage_metadata?.output_tokens ||
            0;
          totalTokensSum +=
            (msg as AIMessage | AIMessageChunk).usage_metadata?.total_tokens ||
            0;
        }
      }

      if (totalTokensSum > 0 && this.context && this.context.logger) {
        this.context.logger.logTokenUsage(
          {
            usage: totalInputTokens,
            service: this.getCredentialTypeForModel(modelConfig.model),
            unit: 'input_tokens',
            subService: modelConfig.model as CredentialType,
          },
          `LLM completion: ${totalInputTokens} input`,
          {
            bubbleName: 'ai-agent',
            variableId: this.context?.variableId,
            operationType: 'bubble_execution',
          }
        );
        this.context.logger.logTokenUsage(
          {
            usage: totalOutputTokens,
            service: this.getCredentialTypeForModel(modelConfig.model),
            unit: 'output_tokens',
            subService: modelConfig.model as CredentialType,
          },
          `LLM completion: ${totalOutputTokens} output`,
          {
            bubbleName: 'ai-agent',
            variableId: this.context?.variableId,
            operationType: 'bubble_execution',
          }
        );
      }

      const response = finalMessage?.content || '';

      // Use shared formatting method
      const formattedResult = formatFinalResponse(
        response,
        modelConfig.model,
        jsonMode
      );
      // If there's an error from formatting (e.g., invalid JSON), return early
      if (formattedResult.error) {
        // Notify executionMeta callback for agent-level errors (e.g. PostHog tracking)
        this.context?.executionMeta?._onAgentError?.({
          error: formattedResult.error,
          model: modelConfig.model,
          iterations,
          toolCalls: toolCalls.length > 0 ? toolCalls : [],
          conversationHistory: this.params.conversationHistory?.map((m) => ({
            role: m.role,
            content: m.content,
          })),
          variableId: this.context?.variableId,
        });

        return {
          response: formattedResult.response,
          toolCalls: toolCalls.length > 0 ? toolCalls : [],
          iterations,
          error: formattedResult.error,
          success: false,
        };
      }

      const finalResponse = formattedResult.response;

      console.log(
        '[AIAgent] Final response length:',
        typeof finalResponse === 'string'
          ? finalResponse.length
          : JSON.stringify(finalResponse).length
      );
      console.log('[AIAgent] Tool calls made:', toolCalls.length);
      console.log(
        '[AIAgent] Execution completed with',
        iterations,
        'iterations'
      );

      return {
        response:
          typeof finalResponse === 'string'
            ? finalResponse
            : JSON.stringify(finalResponse),
        toolCalls: toolCalls.length > 0 ? toolCalls : [],
        iterations,
        error: '',
        success: true,
      };
    } catch (error) {
      console.warn('[AIAgent] Execution error (continuing):', error);
      console.log('[AIAgent] Tool calls before error:', toolCalls.length);
      console.log('[AIAgent] Iterations before error:', iterations);

      // Model fallback logic - only retry if this config has a backup model
      if (modelConfig.backupModel) {
        console.log(
          `[AIAgent] Retrying with backup model: ${modelConfig.backupModel.model}`
        );
        this.context?.logger?.warn(
          `Primary model ${modelConfig.model} failed: ${error instanceof Error ? error.message : 'Unknown error'}. Retrying with backup model... ${modelConfig.backupModel.model}`
        );
        this.streamingCallback?.({
          type: 'error',
          data: {
            error: `Primary model ${modelConfig.model} failed: ${error instanceof Error ? error.message : 'Unknown error'}. Retrying with backup model... ${modelConfig.backupModel.model}`,
            recoverable: true,
          },
        });
        const backupModelConfig = this.buildModelConfig(
          modelConfig,
          modelConfig.backupModel
        );
        const backupResult = await this.executeWithModel(backupModelConfig);
        return backupResult;
      }

      const errorMessage =
        error instanceof Error ? error.message : 'Unknown error';

      // Notify executionMeta callback for agent-level errors (e.g. PostHog tracking)
      this.context?.executionMeta?._onAgentError?.({
        error: errorMessage,
        model: modelConfig.model,
        iterations,
        toolCalls: toolCalls.length > 0 ? toolCalls : [],
        conversationHistory: this.params.conversationHistory?.map((m) => ({
          role: m.role,
          content: m.content,
        })),
        variableId: this.context?.variableId,
      });

      // Return partial results to allow execution to continue
      // Include any tool calls that were completed before the error
      return {
        response: `Execution error: ${errorMessage}`,
        success: false, // Still false but don't completely halt execution
        iterations,
        toolCalls: toolCalls.length > 0 ? toolCalls : [], // Preserve completed tool calls
        error: errorMessage,
      };
    }
  }
}
